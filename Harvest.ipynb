{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cec957-8c7a-4d83-92ed-ec47aabb6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import ParamSpecKwargs\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "# Set up YouTube Data API access\n",
    "api_key = \"Your API Key\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "\n",
    "#Extract Channel data using channel_id copied from a specific YouTube channel\n",
    "def extract_channel_data(channel_id):\n",
    "    print(\"Extracting Channel Data\")\n",
    "    request=youtube.channels().list(\n",
    "                    part=\"snippet,ContentDetails,statistics,status,topicDetails\",\n",
    "                    id=channel_id\n",
    "    )\n",
    "    response=request.execute()\n",
    "\n",
    "    for i in response['items']:\n",
    "        channel_data=dict(Channel_Name=i[\"snippet\"][\"title\"],\n",
    "                Channel_Id=i[\"id\"],\n",
    "                Channel_Status=i['status']['privacyStatus'],\n",
    "                Views=i[\"statistics\"][\"viewCount\"],\n",
    "                Total_Videos=i[\"statistics\"][\"videoCount\"],\n",
    "                Channel_type = i['topicDetails']['topicCategories'],\n",
    "                Channel_Description=i[\"snippet\"][\"description\"],\n",
    "                Playlist_Id=i[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"])\n",
    "    return channel_data\n",
    "\n",
    "\n",
    "#Extract video_ids of the YouTube channel using the playlist_id\n",
    "def extract_video_ids(channel_Id):\n",
    "    print(\"Extracting Videos ID's\")\n",
    "    video_ids=[]\n",
    "    response=youtube.channels().list(id=channel_Id,\n",
    "                                    part='contentDetails').execute()\n",
    "    Playlist_Id=response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    next_page_token=None\n",
    "\n",
    "    while True:\n",
    "        response1=youtube.playlistItems().list(\n",
    "                                            part='snippet',\n",
    "                                            playlistId=Playlist_Id,\n",
    "                                            maxResults=50,\n",
    "                                            pageToken=next_page_token).execute()\n",
    "        for i in range(len(response1['items'])):\n",
    "            video_ids.append(response1['items'][i]['snippet']['resourceId']['videoId'])\n",
    "        next_page_token=response1.get('nextPageToken')\n",
    "\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "#Extract details of all the videos in the YouTube channel using the video_ids\n",
    "def extract_video_data(video_Ids):\n",
    "    print(\"Extracting Videos Data\")\n",
    "    video_data=[]\n",
    "    for video_id in video_Ids:\n",
    "        request=youtube.videos().list(\n",
    "            part=\"snippet,ContentDetails,statistics\",\n",
    "            id=video_id\n",
    "        )\n",
    "        response=request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            data=dict(\n",
    "                    Channel_Name=item['snippet']['channelTitle'],\n",
    "                    Channel_Id=item['snippet']['channelId'],\n",
    "                    Video_Id=item['id'],\n",
    "                    Video_name=item['snippet']['title'],\n",
    "                    Description=item['snippet'].get('description'),\n",
    "                    Published_Date=item['snippet']['publishedAt'],\n",
    "                    Views=item['statistics'].get('viewCount'),\n",
    "                    Likes=item['statistics'].get('likeCount'),\n",
    "                    Favorite_Count=item['statistics']['favoriteCount'],\n",
    "                    Comments=item['statistics'].get('commentCount'),\n",
    "                    Duration=convert_to_minutes(item['contentDetails']['duration']),\n",
    "                    # Tags=item['snippet'].get('tags'),\n",
    "                    Thumbnail=item['snippet']['thumbnails']['default']['url'],\n",
    "                    # Definition=item['contentDetails']['definition'],\n",
    "                    Caption_Status=item['contentDetails']['caption'],\n",
    "                    )\n",
    "            video_data.append(data)    \n",
    "    return video_data\n",
    "\n",
    "# Converting the Duration of the video in minutes for easy processing of the data in database\n",
    "def convert_to_minutes(time_string):\n",
    "    hour_match = re.match(r'PT(?P<hours>\\d+)H(?P<minutes>\\d+)M(?P<seconds>\\d+)S', time_string)\n",
    "    hour_min_match = re.match(r'PT(?P<hours>\\d+)H(?P<minutes>\\d+)M', time_string)\n",
    "    min_sec_match = re.match(r'PT(?P<minutes>\\d+)M(?P<seconds>\\d+)S', time_string)\n",
    "    hour_sec_match = re.match(r'PT(?P<hours>\\d+)H(?P<seconds>\\d+)S', time_string)\n",
    "    hour_only_match = re.match(r'PT(?P<hours>\\d+)H', time_string)\n",
    "    minute_match = re.match(r'PT(?P<minutes>\\d+)M', time_string)\n",
    "    sec_match = re.match(r'PT(?P<seconds>\\d+)S', time_string)\n",
    "\n",
    "    if hour_match:\n",
    "        hours = int(hour_match.group('hours')) if hour_match.group('hours') else 0\n",
    "        minutes = int(hour_match.group('minutes')) if hour_match.group('minutes') else 0\n",
    "        seconds = int(hour_match.group('seconds')) if hour_match.group('seconds') else 0\n",
    "        return hours * 60 + minutes + seconds / 60\n",
    "    elif hour_min_match:\n",
    "        hours = int(hour_min_match.group('hours')) if hour_min_match.group('hours') else 0\n",
    "        minutes = int(hour_min_match.group('minutes')) if hour_min_match.group('minutes') else 0\n",
    "        return hours * 60 + minutes\n",
    "    elif min_sec_match:\n",
    "        minutes = int(min_sec_match.group('minutes')) if min_sec_match.group('minutes') else 0\n",
    "        seconds = int(min_sec_match.group('seconds')) if min_sec_match.group('seconds') else 0\n",
    "        return minutes + seconds / 60\n",
    "    elif hour_sec_match:\n",
    "        hours = int(hour_sec_match.group('hours')) if hour_sec_match.group('hours') else 0\n",
    "        seconds = int(hour_sec_match.group('seconds')) if hour_sec_match.group('seconds') else 0\n",
    "        return hours * 60 + seconds / 60\n",
    "    elif hour_only_match:\n",
    "        hours = int(hour_only_match.group('hours')) if hour_only_match.group('hours') else 0\n",
    "        return hours * 60\n",
    "    elif minute_match:\n",
    "        minutes = int(minute_match.group('minutes')) if minute_match.group('minutes') else 0\n",
    "        return minutes\n",
    "    elif sec_match:\n",
    "        seconds = int(sec_match.group('seconds')) if sec_match.group('seconds') else 0\n",
    "        return seconds / 60\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "#Extract the first 50 comments of each video in the YouTube channel using the video_ids\n",
    "def extract_comments(video_ids):\n",
    "    print(\"Extract Comments Data\")\n",
    "    Comment_data=[]\n",
    "    try:\n",
    "        for video_id in video_ids:\n",
    "            request=youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=50\n",
    "            )\n",
    "            response=request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                data=dict(Comment_Id=item['snippet']['topLevelComment']['id'],\n",
    "                        Video_Id=item['snippet']['topLevelComment']['snippet']['videoId'],\n",
    "                        Comment_Text=item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "                        Comment_Author=item['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                        Comment_Published=item['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "                \n",
    "                Comment_data.append(data)\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return Comment_data\n",
    "\n",
    "#Extract all the data of the YouTube channel(Channel_data, Video_data, Comment_data) by calling the functions\n",
    "def extract_data(id):\n",
    "    print(\"Beginning Data Extraction\")\n",
    "    channel_id = id\n",
    "    channel_data = extract_channel_data(channel_id)\n",
    "    video_ids = extract_video_ids(channel_id)\n",
    "    video_data = []\n",
    "    comment_data = []\n",
    "    video_data = extract_video_data(video_ids)\n",
    "    comment_data = extract_comments(video_ids)\n",
    "    # video_data.append(video_details)\n",
    "    # comment_data.append(video_details['Comments'])\n",
    "    video_df = pd.DataFrame(video_data)\n",
    "    comment_df = pd.DataFrame(comment_data)\n",
    "    return(channel_data,video_df,comment_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
